---
layout: post
title: NLP_文本分类,聚类
categories: Machine-Learning NLP
tags: Machine-Learning NLP
author: kaiyu
---

* content
 {:toc}

这也是很老的话题了, 写在这里也是自己的一个记录. 给定分类的类别以及具有标签的训练数据来训练一个模型,使其具有对未知文本分类的能力,称之为文本分类. 只指定簇的数据而且用于训练数据无标签,将文本集合自动聚类成多个簇,使得同一个簇中的文本内容具有较高的相似度,而不同簇相似度较低,这个过程称之为文本聚类.




## 文本分类
文本分类方法主要指自动文本分类,包括两种方法:
1. 知识工程: 由专家定义规则, 计算机根据这些手工指定的规则来对文本进行分类.
2. 机器学习: 根据统计原理,使用不同的模型,经过训练后完成从原始文本数据到类别的映射(或者降维)
### 文本分类主要分为三个步骤:
#### 1. 将文本转换成机器能够识别的形式. 
最常用的方法是向量空间模型:将文本表示为文档-词矩阵,每一行代表一个文档向量,每个元素代表某个词在对应文档中的权重. 选取哪些词作为基准,成为特征选择,权重的计算也有很多方法.常见的特征选择方法有:文档频率,信息增益,互信息,期望交叉熵等等.
#### 2. 分类器的构建
这个主要是模型的选择,模型是很多的,而且适合的场景也是千差万别.
1. Rocchio方法:  
每一类确定一个中心点（centroid），计算待分类的文档与各类代表元间的距离，并作为判定是否属于该类的判据。Rocchio方法最早由[Hull, 1994]引入文本分类领域，后来又有很多文章进行了改进。Rocchio方法的特点是容易实现，效率高。缺点是受文本集分布的影响，比如计算出的中心点可能落在相应的类别之外[Sebastiani, 2002]。
2. 朴素贝叶斯方法:  
使用贝叶斯公式，通过先验概率和类别的条件概率来估计文档对某一类别的后验概率，以此实现对此文档所属类别的判断。[Lewis, 1998]介绍了朴素贝叶斯方法的发展和各种变体及特点。
3. k邻近算法(knn):  
从训练集中找出与待分类文档最近的k个邻居（文档），根据这k个邻居的类别来决定待分类文档的类别。KNN方法的优点是不需要特征选取和训练，很容易处理类别数目多的情况，缺点之一是空间复杂度高。KNN方法得到的分类器是非线性分类器。此方法最早由[Yang & Chute, 1994]提出。
4. 支持向量机方法(SVM):  
 对于某个类别，找出一个分类面，使得这个类别的正例和反例落在这个分类面的两侧，而且这个分类面满足：到最近的正例和反例的距离相等，而且是所有分类面中与正例（或反例）距离最大的一个分类面。SVM方法最早由[Joachims, 1998]引入到文本分类中。SVM方法的优点是使用很少的训练集，计算量小；缺点是太依赖于分类面附近的正例和反例的位置，具有较大的偏执。
5. 决策树: 
 决策树通过再每个节点通过某个特征来对样本进行分离操作, 节点特征的选取通常是通过信息熵和信息增益来确定.也就是说通过每个节点的不同特征,决策树把数据集逐步划分为越来越小的子树.决策树经常和提升算法结合使用,如梯度提升树. 

#### 3. 效果评估
评估过程基于训练集,和一般的机器学习算法的评估并无两样

## 文本聚类   
根据聚成的簇的特点，聚类技术通常分为层次聚类（hierarchical clustering）和划分聚类（partitional clustering）。前者比较典型的例子是凝聚（agglomerative）层次聚类算法，后者的典型例子是k-means算法。近年来出现了一些新的聚类算法，它们基于不同的理论或技术，比如图论，模糊集理论，神经网络以及核技术（kernel techniques）等等。
### 文本聚类大致也可以分为三个步骤  
#### 1. 文本表示  
这里和文本分类并无二样
#### 2. 聚类算法  
算法的选择，往往伴随着相似度计算方法的选择。在文本挖掘中，最常用的相似度计算方法是余弦相似度。聚类算法有很多种，但是没有一个通用的算法可以解决所有的聚类问题。
1. 层次聚类方法:  
层次聚类可以分为两种：凝聚（agglomerative）层次聚类和划分（divisive）层次聚类.凝聚方法把每个文本作为一个初始簇,经过不断的合并过程,最后成为一个簇.划分方法的过程正好与之相反.划分方法在现实中采用较少，有关论述请见[Kaufman & Rousseeuw, 1990].层次聚类可以得到层次化的聚类结果，但是计算复杂度比较高，不能处理大量的文档.
2. 划分方法:  
k-means算法是最常见的划分方法。给定簇的个数k，选定k个文本分别作为k个初始簇，将其他的文本加入最近的簇中，并更新簇的中心点，然后再根据新的中心点对文本重新划分；当簇不再变化时或经过一定次数的迭代之后，算法停止。k-means算法复杂度低，而且容易实现，但是对例外和噪声文本比较敏感。另外一个问题是，k的取值较为困难.
3. 基于密度的方法:  
为了发现任意形状的聚类结果，提出了基于密度的方法。这类方法将簇看作是数据空间中被低密度区域分割开的高密度区域。常见的基于密度的方法有DBSCAN, OPTICS, DENCLUE等等，参考文献见[Han & Kamber, 2006].
4. 神经网络方法:  
神经网络方法将每个簇描述为一个标本，标本作为聚类的”原型”，不一定对应一个特定的数据,根据某些距离度量，新的对象被分配到与其最相似的簇中。比较著名的神经网络聚类算法有:竞争学习（competitive learing）和自组织特征映射（self-organizing map）[Kohonen, 1990].
5. 概率聚类和主题模型:  
主题建模是最流行的一种概率聚类算法，近来受到广泛关注。主题建模的主要思想是为文本文档的语料构建概率生成模型。在主题模型中，文档是主题的混合体，而主题则是单词的概率分布。  
两种主要的主题模型分别为：概率潜在语义分析（Probabilistic Latent Semantic Analysis/pLSA） 和隐狄利克雷分布（LDA)）。pLSA 模型在文档层面不提供任何概率模型，这使得很难泛化到新的没见过的文档。  
隐狄利克雷分配模型是最新的无监督技术，用于提取所收集文档的专题信息（主题）。其基础思想为文档是潜在主题的随机混合，每个主题为单词的概率分布
#### 4. 聚类评估  
因为没有具有标签的测试数据进行效果的评估,因此对聚类算法最终的效果进行评估是困难的. 在这里一般可以用手工或者已经标记好的数据作为测试数据来对算法聚类的簇进行评估.
